{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ayyucedemirbas/DoRA/blob/main/dora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7ef0918d70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/ayyucedemirbas/DoRA/blob/main/dora.py\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This layer is dropped into your pre-trained PyTorch model where nn.Linear is used\n",
    "class DoRALayer(nn.Module):\n",
    "    def __init__(self, d_in, d_out, rank=4, weight=None, bias=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if weight is not None:\n",
    "            self.weight = nn.Parameter(weight, requires_grad=False)\n",
    "        else:\n",
    "            self.weight = nn.Parameter(torch.Tensor(d_out, d_in), requires_grad=False)\n",
    "\n",
    "        if bias is not None:\n",
    "            self.bias = nn.Parameter(bias, requires_grad=False)\n",
    "        else:\n",
    "            self.bias = nn.Parameter(torch.Tensor(d_out), requires_grad=False)\n",
    "\n",
    "        # m = Magnitude column-wise across output dimension\n",
    "        self.m = nn.Parameter(self.weight.norm(p=2, dim=0, keepdim=True))\n",
    "        \n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.lora_A = nn.Parameter(torch.randn(d_out, rank)*std_dev)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, d_in))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora = torch.matmul(self.lora_A, self.lora_B)\n",
    "        adapted = self.weight + lora\n",
    "        column_norm = adapted.norm(p=2, dim=0, keepdim=True)\n",
    "        norm_adapted = adapted / column_norm\n",
    "        calc_weights = self.m * norm_adapted\n",
    "        return F.linear(x, calc_weights, self.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, criterion, optimizer, data_loader, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, targets in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "def replace_linear_with_dora(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Get the input and output dimensions of the current nn.Linear layer\n",
    "            d_in = module.in_features\n",
    "            d_out = module.out_features\n",
    "\n",
    "            # Create a new DoRALayer with the same dimensions\n",
    "            setattr(model, name, DoRALayer(d_out=d_out, d_in=d_in, weight=module.weight.data.clone(), bias=module.bias.data.clone()))\n",
    "        else:\n",
    "            # Recursively apply this function to submodules\n",
    "            replace_linear_with_dora(module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples=100, input_dim=10):\n",
    "    X = np.random.randn(num_samples, input_dim).astype(np.float32)\n",
    "    y = np.sum(X, axis=1, keepdims=True)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader():\n",
    "    # Dados textuais de exemplo\n",
    "    texts = [\"I love machine learning\", \"Deep learning is amazing\", \"Natural language processing\"]\n",
    "    labels = [0, 1, 0]\n",
    "\n",
    "    # Usar CountVectorizer para converter texto em vetores de contagem\n",
    "    vectorizer = CountVectorizer()\n",
    "    data = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "    # Converter dados e r√≥tulos para tensores\n",
    "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    # Criar TensorDataset\n",
    "    dataset = TensorDataset(data_tensor, labels_tensor)\n",
    "\n",
    "    # Usar DataLoader para iterar pelo dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    # Iterar pelo DataLoader\n",
    "    for batch_data, batch_labels in dataloader:\n",
    "        print(batch_data, batch_labels)\n",
    "\n",
    "    return dataloader, data_tensor.shape[1], len(set(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 1., 1.]]) tensor([0., 0.])\n",
      "tensor([[1., 1., 1., 0., 1., 0., 0., 0., 0.]]) tensor([1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanessa/.pyenv/versions/3.11.7/envs/dora/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/vanessa/.pyenv/versions/3.11.7/envs/dora/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 20\n",
      "Trainable Parameters: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanessa/.pyenv/versions/3.11.7/envs/dora/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/vanessa/.pyenv/versions/3.11.7/envs/dora/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "data_loader, input_dim, output_dim = get_data_loader()\n",
    "\n",
    "model = SimpleModel(input_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "#X, y = generate_data(num_samples=1000, input_dim=input_dim)\n",
    "#dataset = TensorDataset(X, y)\n",
    "#data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "print_model_parameters(model)\n",
    "\n",
    "train(model, criterion, optimizer, data_loader, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation Loss: 0.15919075906276703\n",
      "Total Parameters: 73\n",
      "Trainable Parameters: 53\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs, targets = next(iter(data_loader))\n",
    "    predictions = model(inputs)\n",
    "    loss = criterion(predictions, targets)\n",
    "    print(f\"Final Evaluation Loss: {loss.item()}\")\n",
    "\n",
    "replace_linear_with_dora(model)\n",
    "\n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing training with DoRA layers...\n",
      "INPUTS tensor([[0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 1., 1.]])\n",
      "predictions tensor([[0.1503, 0.1500],\n",
      "        [0.0704, 0.4674]])\n",
      "Final (DoRA) Evaluation Loss: 0.06712991744279861\n"
     ]
    }
   ],
   "source": [
    "# Continue training with the Dora model\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "print(\"Continuing training with DoRA layers...\")\n",
    "train(model, criterion, optimizer, data_loader, epochs=5)  # Continue training\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs, targets = next(iter(data_loader))\n",
    "    predictions = model(inputs)\n",
    "    print(\"INPUTS\", inputs)\n",
    "    print(\"predictions\",  predictions)\n",
    "    loss = criterion(predictions, targets)\n",
    "    print(f\"Final (DoRA) Evaluation Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
